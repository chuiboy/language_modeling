{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling: Word level\n",
    "___\n",
    "#### Description: \n",
    "In this notebook I train a recurrent neural network (RNN) on Moby Dick and use the model to generate new text. Like the character-level model, I use a many-to-one RNN where the input is a sequence of words and the output is the word that follows the sequence. \n",
    "___\n",
    "#### Dataset: \n",
    "The dataset consists of the text for Moby Dick.\n",
    "___\n",
    "#### Reference: \n",
    "\n",
    "This notebook was completed using the following resources as a guide:\n",
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MOBY-DICK \n",
      "\n",
      "CHAPTER I \n",
      "\n",
      "LOOMINGS \n",
      "\n",
      "CALL me Ishmael. Some years ago never mind how \n",
      "long precisely having little or no money in my purse, \n",
      "and nothing particular to interest me on shore, I thought \n",
      "I would sail about a little and see the watery part of the \n",
      "world. It is a way I have of driving off the spleen, and \n",
      "regulating the circulation. Whenever I find myself \n",
      "growing grim about the mouth ; whenever it is a damp, \n",
      "drizzly November in my soul ; whenever I find myself \n",
      "involuntarily pausing b\n"
     ]
    }
   ],
   "source": [
    "# Read in the text\n",
    "with open('Datasets/mobydick.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBy skimming over the text we can see what needs to be cleaned. For example, the text includes \\nheaders/footers, page numbers, volume numbers, chapter titles, etc. These should be removed.\\nAs for punctuation, I will remove all instances of \"- \\n\" since they usually indicate a word\\nhas been split to a new line. I will also remove punctuation except for: \\'.\\', \\',\\', \\';\\', \\'?\\', \\'!\\' \\nwhich will be replaced with: <eos>, <comma>, <semicolon>, <question>, <exclamation>. \\nNumbers will also be removed and words will all be made lowercase.\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning the text\n",
    "\"\"\"\n",
    "By skimming over the text we can see what needs to be cleaned. For example, the text includes \n",
    "headers/footers, page numbers, volume numbers, chapter titles, etc. These should be removed.\n",
    "As for punctuation, I will remove all instances of \"- \\n\" since they usually indicate a word\n",
    "has been split to a new line. I will also remove punctuation except for: '.', ',', ';', '?', '!' \n",
    "which will be replaced with: <eos>, <comma>, <semicolon>, <question>, <exclamation>. \n",
    "Numbers will also be removed and words will all be made lowercase.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "import re\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    text = re.sub('(\\n[0-9A-Z -]+.{0,12} \\n)', '',text) # Remove headers/footers\n",
    "    text = text.replace('Mr.', 'Mr') # Remove '.' from Mr. \n",
    "    text = text.replace('Mrs.', 'Mrs') # Remove '.' from Mrs.\n",
    "    text = text.replace(';', ' <semicolon> ') # Replace ';' with <semicolon>\n",
    "    text = text.replace('.', ' <eos> ') # Replace '.' with <eos>\n",
    "    text = text.replace(',', ' <comma> ') # Replace ',' with <comma>\n",
    "    text = text.replace('?', ' <question> ') # Replace '?' with <question>\n",
    "    text = text.replace('!', ' <exclamation> ') # Replace '!' with <exclamation>\n",
    "    text = text.replace('- \\n', '') # Fix divided words\n",
    "    text = re.sub('[^a-zA-Z<>\\s]', '', text) # Remove remaining punctuation\n",
    "    text = text.lower() # Make all text lowercase\n",
    "    text = text.split() # split text to a list of words\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['call', 'me', 'ishmael', '<eos>', 'some', 'years', 'ago', 'never', 'mind', 'how', 'long', 'precisely', 'having', 'little', 'or', 'no', 'money', 'in', 'my', 'purse', '<comma>', 'and', 'nothing', 'particular', 'to', 'interest', 'me', 'on', 'shore', '<comma>', 'i', 'thought', 'i', 'would', 'sail', 'about', 'a', 'little', 'and', 'see', 'the', 'watery', 'part', 'of', 'the', 'world', '<eos>', 'it', 'is', 'a', 'way', 'i', 'have', 'of', 'driving', 'off', 'the', 'spleen', '<comma>', 'and', 'regulating', 'the', 'circulation', '<eos>', 'whenever', 'i', 'find', 'myself', 'growing', 'grim', 'about', 'the', 'mouth', '<semicolon>', 'whenever', 'it', 'is', 'a', 'damp', '<comma>', 'drizzly', 'november', 'in', 'my', 'soul', '<semicolon>', 'whenever', 'i', 'find', 'myself', 'involuntarily', 'pausing', 'before', 'coffin', 'warehouses', '<comma>', 'and', 'bringing', 'up', 'the']\n"
     ]
    }
   ],
   "source": [
    "# Clean and tokenize the text\n",
    "text = clean_and_tokenize(text)\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 12503\n"
     ]
    }
   ],
   "source": [
    "# Extract all unique words from text\n",
    "words = sorted(list(set(text)))\n",
    "print('Unique words:', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries that maps words to indices and vice versa\n",
    "word_indices = dict((w, i+1) for i, w in enumerate(words))\n",
    "indices_word = dict((i+1, c) for i, c in enumerate(words))\n",
    "# 0th index for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each word to an integer using word_indices\n",
    "word_ind = [word_indices[word] for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1507, 6657, 5784, 2, 10034, 12466, 242, 7154, 6815, 5219, 6359, 8182, 4924, 6312, 7440, 7193, 6894, 5439, 7042, 8492, 1, 387, 7243, 7650, 11128, 5686, 6657, 7405, 9681, 1, 5300, 11008, 5300, 12408, 9202, 44, 11, 6312, 387, 9448, 10947, 12046, 7644, 7352, 10947, 12387, 2, 5800, 5782, 11, 12055, 5300, 4921, 7352, 3242, 7353, 10947, 10169, 1, 387, 8775, 10947, 1861, 2, 12185, 5300, 4020, 7043, 4732, 4701, 44, 10947, 6971, 5, 12185, 5800, 5782, 11, 2648, 1, 3243, 7255, 5439, 7042, 10068, 5, 12185, 5300, 4020, 7043, 5758, 7701, 917, 2006, 11996, 1, 387, 1348, 11707, 10947]\n"
     ]
    }
   ],
   "source": [
    "# First 100 words in the text as integers\n",
    "print(word_ind[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of sequence (input + output)\n",
    "maxlen = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "sequences = []\n",
    "for i in range(0, len(word_ind)-maxlen):\n",
    "    sequence = word_ind[i:maxlen+i]\n",
    "    sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#Sequences, Sequence length) -> (120245, 51)\n"
     ]
    }
   ],
   "source": [
    "# Convert list to numpy array\n",
    "import numpy as np\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "print('(#Sequences, Sequence length) ->', sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 12504\n"
     ]
    }
   ],
   "source": [
    "# Number of unique characters + 1\n",
    "vocab_size = len(words) + 1 # including 0th index\n",
    "print('Vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (120245, 50)\n",
      "y shape: (120245, 12504)\n"
     ]
    }
   ],
   "source": [
    "# Split sequences into inputs and outputs\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size) # convert y to one-hot vector\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            625200    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12504)             3213528   \n",
      "=================================================================\n",
      "Total params: 4,005,384\n",
      "Trainable params: 4,005,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build a RNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=X.shape[1]),\n",
    "    LSTM(100, return_sequences=True),\n",
    "    LSTM(100),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')]\n",
    ")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "120245/120245 [==============================] - 1928s 16ms/step - loss: 6.4995 - acc: 0.0949\n",
      "Epoch 2/100\n",
      "120245/120245 [==============================] - 3982s 33ms/step - loss: 6.0820 - acc: 0.1178\n",
      "Epoch 3/100\n",
      "120245/120245 [==============================] - 3599s 30ms/step - loss: 5.8844 - acc: 0.1261\n",
      "Epoch 4/100\n",
      "120245/120245 [==============================] - 3512s 29ms/step - loss: 5.7337 - acc: 0.1334\n",
      "Epoch 5/100\n",
      "120245/120245 [==============================] - 3502s 29ms/step - loss: 5.5866 - acc: 0.1407\n",
      "Epoch 6/100\n",
      "120245/120245 [==============================] - 3145s 26ms/step - loss: 5.4779 - acc: 0.1462\n",
      "Epoch 7/100\n",
      "120245/120245 [==============================] - 3161s 26ms/step - loss: 5.3929 - acc: 0.1517\n",
      "Epoch 8/100\n",
      "120245/120245 [==============================] - 3243s 27ms/step - loss: 5.2941 - acc: 0.1581\n",
      "Epoch 9/100\n",
      "120245/120245 [==============================] - 3292s 27ms/step - loss: 5.2014 - acc: 0.1633\n",
      "Epoch 10/100\n",
      "120245/120245 [==============================] - 3149s 26ms/step - loss: 5.1143 - acc: 0.1691\n",
      "Epoch 11/100\n",
      "120245/120245 [==============================] - 3299s 27ms/step - loss: 5.0309 - acc: 0.1730\n",
      "Epoch 12/100\n",
      "120245/120245 [==============================] - 3754s 31ms/step - loss: 4.9526 - acc: 0.1774\n",
      "Epoch 13/100\n",
      "120245/120245 [==============================] - 3754s 31ms/step - loss: 4.8824 - acc: 0.1816\n",
      "Epoch 14/100\n",
      "120245/120245 [==============================] - 3711s 31ms/step - loss: 4.8145 - acc: 0.1850\n",
      "Epoch 15/100\n",
      "120245/120245 [==============================] - 3407s 28ms/step - loss: 4.7503 - acc: 0.1888\n",
      "Epoch 16/100\n",
      "120245/120245 [==============================] - 3478s 29ms/step - loss: 4.6812 - acc: 0.1922\n",
      "Epoch 17/100\n",
      "120245/120245 [==============================] - 3401s 28ms/step - loss: 4.6158 - acc: 0.1960\n",
      "Epoch 18/100\n",
      "120245/120245 [==============================] - 3425s 28ms/step - loss: 4.5481 - acc: 0.1992\n",
      "Epoch 19/100\n",
      "120245/120245 [==============================] - 3602s 30ms/step - loss: 4.4796 - acc: 0.2029\n",
      "Epoch 20/100\n",
      "120245/120245 [==============================] - 3411s 28ms/step - loss: 4.4118 - acc: 0.2056\n",
      "Epoch 21/100\n",
      "120245/120245 [==============================] - 3451s 29ms/step - loss: 4.3450 - acc: 0.2102\n",
      "Epoch 22/100\n",
      "120245/120245 [==============================] - 3397s 28ms/step - loss: 4.2773 - acc: 0.2134\n",
      "Epoch 23/100\n",
      "120245/120245 [==============================] - 3487s 29ms/step - loss: 4.2100 - acc: 0.2176\n",
      "Epoch 24/100\n",
      "120245/120245 [==============================] - 3447s 29ms/step - loss: 4.1396 - acc: 0.2223\n",
      "Epoch 25/100\n",
      "120245/120245 [==============================] - 3495s 29ms/step - loss: 4.0680 - acc: 0.2275\n",
      "Epoch 26/100\n",
      "120245/120245 [==============================] - 3695s 31ms/step - loss: 3.9954 - acc: 0.2323\n",
      "Epoch 27/100\n",
      "120245/120245 [==============================] - 3369s 28ms/step - loss: 3.9249 - acc: 0.2387\n",
      "Epoch 28/100\n",
      "120245/120245 [==============================] - 3515s 29ms/step - loss: 3.8514 - acc: 0.2449\n",
      "Epoch 29/100\n",
      "120245/120245 [==============================] - 3493s 29ms/step - loss: 3.7809 - acc: 0.2509\n",
      "Epoch 30/100\n",
      "120245/120245 [==============================] - 3798s 32ms/step - loss: 3.7116 - acc: 0.2599\n",
      "Epoch 31/100\n",
      "120245/120245 [==============================] - 3332s 28ms/step - loss: 3.6427 - acc: 0.2668\n",
      "Epoch 32/100\n",
      "120245/120245 [==============================] - 3295s 27ms/step - loss: 3.5773 - acc: 0.2746\n",
      "Epoch 33/100\n",
      "120245/120245 [==============================] - 3287s 27ms/step - loss: 3.5147 - acc: 0.2812\n",
      "Epoch 34/100\n",
      "120245/120245 [==============================] - 3493s 29ms/step - loss: 3.4521 - acc: 0.2896\n",
      "Epoch 35/100\n",
      "120245/120245 [==============================] - 3393s 28ms/step - loss: 3.3925 - acc: 0.2973\n",
      "Epoch 36/100\n",
      "120245/120245 [==============================] - 3475s 29ms/step - loss: 3.3307 - acc: 0.3063\n",
      "Epoch 37/100\n",
      "120245/120245 [==============================] - 3755s 31ms/step - loss: 3.2785 - acc: 0.3130\n",
      "Epoch 38/100\n",
      "120245/120245 [==============================] - 3558s 30ms/step - loss: 3.2228 - acc: 0.3220\n",
      "Epoch 39/100\n",
      "120245/120245 [==============================] - 3789s 32ms/step - loss: 3.1740 - acc: 0.3280\n",
      "Epoch 40/100\n",
      "120245/120245 [==============================] - 3410s 28ms/step - loss: 3.1158 - acc: 0.3362\n",
      "Epoch 41/100\n",
      "120245/120245 [==============================] - 3434s 29ms/step - loss: 3.0639 - acc: 0.3452\n",
      "Epoch 42/100\n",
      "120245/120245 [==============================] - 3418s 28ms/step - loss: 3.0176 - acc: 0.3523\n",
      "Epoch 43/100\n",
      "120245/120245 [==============================] - 3437s 29ms/step - loss: 2.9658 - acc: 0.3594\n",
      "Epoch 44/100\n",
      "120245/120245 [==============================] - 3423s 28ms/step - loss: 2.9186 - acc: 0.3665\n",
      "Epoch 45/100\n",
      "120245/120245 [==============================] - 3428s 29ms/step - loss: 2.8701 - acc: 0.3744\n",
      "Epoch 46/100\n",
      "120245/120245 [==============================] - 3385s 28ms/step - loss: 2.8214 - acc: 0.3812\n",
      "Epoch 47/100\n",
      "120245/120245 [==============================] - 3415s 28ms/step - loss: 2.7791 - acc: 0.3884\n",
      "Epoch 48/100\n",
      "120245/120245 [==============================] - 3441s 29ms/step - loss: 2.7369 - acc: 0.3962\n",
      "Epoch 49/100\n",
      "120245/120245 [==============================] - 3482s 29ms/step - loss: 2.6924 - acc: 0.4029\n",
      "Epoch 50/100\n",
      "120245/120245 [==============================] - 3436s 29ms/step - loss: 2.6501 - acc: 0.4104\n",
      "Epoch 51/100\n",
      "120245/120245 [==============================] - 3682s 31ms/step - loss: 2.6068 - acc: 0.4168\n",
      "Epoch 52/100\n",
      "120245/120245 [==============================] - 3559s 30ms/step - loss: 2.5694 - acc: 0.4241\n",
      "Epoch 53/100\n",
      "120245/120245 [==============================] - 3517s 29ms/step - loss: 2.5301 - acc: 0.4298\n",
      "Epoch 54/100\n",
      "120245/120245 [==============================] - 3435s 29ms/step - loss: 2.4900 - acc: 0.4366\n",
      "Epoch 55/100\n",
      "120245/120245 [==============================] - 3502s 29ms/step - loss: 2.4524 - acc: 0.4444\n",
      "Epoch 56/100\n",
      "120245/120245 [==============================] - 3849s 32ms/step - loss: 2.4147 - acc: 0.4502\n",
      "Epoch 57/100\n",
      "120245/120245 [==============================] - 3528s 29ms/step - loss: 2.3794 - acc: 0.4587\n",
      "Epoch 58/100\n",
      "120245/120245 [==============================] - 3874s 32ms/step - loss: 2.3422 - acc: 0.4648\n",
      "Epoch 59/100\n",
      "120245/120245 [==============================] - 3506s 29ms/step - loss: 2.3061 - acc: 0.4689\n",
      "Epoch 60/100\n",
      "120245/120245 [==============================] - 3395s 28ms/step - loss: 2.2685 - acc: 0.4774\n",
      "Epoch 61/100\n",
      "120245/120245 [==============================] - 3692s 31ms/step - loss: 2.2382 - acc: 0.4830\n",
      "Epoch 62/100\n",
      "120245/120245 [==============================] - 3480s 29ms/step - loss: 2.2079 - acc: 0.4893\n",
      "Epoch 63/100\n",
      "120245/120245 [==============================] - 4044s 34ms/step - loss: 2.1731 - acc: 0.4941\n",
      "Epoch 64/100\n",
      "120245/120245 [==============================] - 3803s 32ms/step - loss: 2.1439 - acc: 0.4991\n",
      "Epoch 65/100\n",
      "120245/120245 [==============================] - 3233s 27ms/step - loss: 2.1127 - acc: 0.5065\n",
      "Epoch 66/100\n",
      "120245/120245 [==============================] - 3143s 26ms/step - loss: 2.0835 - acc: 0.5128\n",
      "Epoch 67/100\n",
      "120245/120245 [==============================] - 3172s 26ms/step - loss: 2.0538 - acc: 0.5168\n",
      "Epoch 68/100\n",
      "120245/120245 [==============================] - 3114s 26ms/step - loss: 2.0274 - acc: 0.5222\n",
      "Epoch 69/100\n",
      "120245/120245 [==============================] - 3170s 26ms/step - loss: 1.9965 - acc: 0.5295\n",
      "Epoch 70/100\n",
      "120245/120245 [==============================] - 3187s 27ms/step - loss: 1.9743 - acc: 0.5332\n",
      "Epoch 71/100\n",
      "120245/120245 [==============================] - 3169s 26ms/step - loss: 1.9429 - acc: 0.5391\n",
      "Epoch 72/100\n",
      "120245/120245 [==============================] - 3213s 27ms/step - loss: 1.9264 - acc: 0.5423\n",
      "Epoch 73/100\n",
      "120245/120245 [==============================] - 3217s 27ms/step - loss: 1.8962 - acc: 0.5470\n",
      "Epoch 74/100\n",
      "120245/120245 [==============================] - 3253s 27ms/step - loss: 1.8811 - acc: 0.5523\n",
      "Epoch 75/100\n",
      "120245/120245 [==============================] - 3162s 26ms/step - loss: 1.8530 - acc: 0.5573\n",
      "Epoch 76/100\n",
      "120245/120245 [==============================] - 3407s 28ms/step - loss: 1.8283 - acc: 0.5615\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120245/120245 [==============================] - 3609s 30ms/step - loss: 1.8015 - acc: 0.5678\n",
      "Epoch 78/100\n",
      "120245/120245 [==============================] - 3235s 27ms/step - loss: 1.7877 - acc: 0.5693\n",
      "Epoch 79/100\n",
      "120245/120245 [==============================] - 3380s 28ms/step - loss: 1.7723 - acc: 0.5731\n",
      "Epoch 80/100\n",
      "120245/120245 [==============================] - 3420s 28ms/step - loss: 1.7424 - acc: 0.5784\n",
      "Epoch 81/100\n",
      "120245/120245 [==============================] - 3659s 30ms/step - loss: 1.7285 - acc: 0.5807\n",
      "Epoch 82/100\n",
      "120245/120245 [==============================] - 3453s 29ms/step - loss: 1.7005 - acc: 0.5881\n",
      "Epoch 83/100\n",
      "120245/120245 [==============================] - 3139s 26ms/step - loss: 1.6909 - acc: 0.5889\n",
      "Epoch 84/100\n",
      "120245/120245 [==============================] - 3345s 28ms/step - loss: 1.6716 - acc: 0.5929\n",
      "Epoch 85/100\n",
      "120245/120245 [==============================] - 3420s 28ms/step - loss: 1.6533 - acc: 0.5963\n",
      "Epoch 86/100\n",
      "120245/120245 [==============================] - 3495s 29ms/step - loss: 1.6322 - acc: 0.6023\n",
      "Epoch 87/100\n",
      "120245/120245 [==============================] - 3715s 31ms/step - loss: 1.6179 - acc: 0.6050\n",
      "Epoch 88/100\n",
      "120245/120245 [==============================] - 3762s 31ms/step - loss: 1.5965 - acc: 0.6090\n",
      "Epoch 89/100\n",
      "120245/120245 [==============================] - 3619s 30ms/step - loss: 1.5853 - acc: 0.6114\n",
      "Epoch 90/100\n",
      "120245/120245 [==============================] - 3522s 29ms/step - loss: 1.5621 - acc: 0.6177\n",
      "Epoch 91/100\n",
      "120245/120245 [==============================] - 3558s 30ms/step - loss: 1.5520 - acc: 0.6177\n",
      "Epoch 92/100\n",
      "120245/120245 [==============================] - 3545s 29ms/step - loss: 1.5325 - acc: 0.6228\n",
      "Epoch 93/100\n",
      "120245/120245 [==============================] - 3564s 30ms/step - loss: 1.5262 - acc: 0.6227\n",
      "Epoch 94/100\n",
      "120245/120245 [==============================] - 3557s 30ms/step - loss: 1.5046 - acc: 0.6265\n",
      "Epoch 95/100\n",
      "120245/120245 [==============================] - 3549s 30ms/step - loss: 1.4905 - acc: 0.6302\n",
      "Epoch 96/100\n",
      "120245/120245 [==============================] - 3560s 30ms/step - loss: 1.4772 - acc: 0.6321\n",
      "Epoch 97/100\n",
      "120245/120245 [==============================] - 3481s 29ms/step - loss: 1.4649 - acc: 0.6365\n",
      "Epoch 98/100\n",
      "120245/120245 [==============================] - 3632s 30ms/step - loss: 1.4529 - acc: 0.6395\n",
      "Epoch 99/100\n",
      "120245/120245 [==============================] - 3535s 29ms/step - loss: 1.4362 - acc: 0.6434\n",
      "Epoch 100/100\n",
      "120245/120245 [==============================] - 3579s 30ms/step - loss: 1.4189 - acc: 0.6454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c3cbef0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model to X and y\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Checkpoint weights after every epoch (optional)\n",
    "checkpoint = ModelCheckpoint('weights-{epoch:02d}-{acc:.2f}.hdf5')\n",
    "\n",
    "# Compile\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit\n",
    "model.fit(X, y, epochs=100, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert word(s) from vocab: the whale was\n",
      "======== Generated text ========\n",
      "the whale was tossing away at a cane into its bed, superstitiously form out your bill, god bless ye, he s a very look in swimming out the ship of the tinkling glasses within us like a anchor a sort of badgerhaired as stubb again; but upon the hands were ready to sit her noble dismal night, and lumbered the eyes of which districts were gallantly below by the castle to pacific, and not only cruising ahead and hesitatingly ere such never fancied his ship can possibly have to break his white stool in the open air, and standing in the remotest degree ahab, but never go as if they burst this sleep; but they have left completely made your old wooden whalebone, you, having one look at one tithe of their heart. now he procured down in the transom, and was there he will answer him to see from any wise old is us yet had just thrown after daggoo. more considered his eyes like a whale till altogether business of such lost a second land, he darted down the mighty articles; while, pausing there, like a dumb brute is, too, are very easy, than example, are the old; shipmate pity you a little circumvention for the persian red man, illuminated to the neighbours, which could be so strange an ostrich. it stood by its hanging from the deep, where once circumnavigation sailed off forward, you d. aft! heart. proceed, ye mates. terrors along the tambourine; swinging same cries to these anchor in all black aloft ere an idea to fine s concluded her kingpost, thou? stubb, that s a troubled league. ha! our mark was something to leeward. but tis hot. pull, hearties! that s once as thou art soon not before the anchor, then he pulled, there clean swinging heavy s removed the broad glare in the street; that even it may so seem; the pipe reached his teeth; by these countenance? s had one of the storm neither than bildad, often without the interval aft confidently both whiteness of the unspeakable reciprocating in business. upon old knees the two captains were used and places among the american point of whaling; for the life of the ship came down for the last; in pursuit of his less place, because with a mans christian voyage was steering evinced in a heathenish seas. from down wide noble worlds or headsman in his way, the order to see all these fancies peering into his ivory stool he has not find out for a brisk scolding with a voyage, and more than sperm service, isolatoes woodlands his leg for my bag, but bear again a few phenomenon of vaguely strike him he was pulling so remote out of their same large things we should take the ship. but as how wonder nothing but the whale a curious too as thus an eye in my nantucket market. fatal peep from the oriental crisis of the voyage, the passion which only drew him to what all that girl of truly, when the thunders in revenge when it is. then, both large and great aloft tar in anything old obligations i; who so turned starbuck down there, will i see, the whale was not a present of the sin; which this same plan might be taken, and i have no tiller. and it seems; what exceedingly vast apt to doze through the heart of doom, it would not have washed his cetological long boat within thee; and though, for the uncertain twilight and god was but a sunday allusions of any,, at the brazil whale have not been about his son. was then next man seemed certain engrafted tenement the sperm whale or small concern and half to be a body. because they at last especially lighted me over his own sign of that merciless place, they plainly saw all the very part. consider, you has baleen. but this under all parts which, from my simple hearers their seat from it. by that seemed only to take these thirty feet together. the window, which even his foreboding tattooings, for a greenhand at god, a crew, leaped from the bulwarks, and their hearts this jets seemed spoke of both vast brighter accompaniments, his death was also"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "import sys\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Prompt user input\n",
    "text = input('Insert word(s) from vocab: ')\n",
    "\n",
    "print('======== Generated text ========')\n",
    "sys.stdout.write(text)\n",
    "\n",
    "# Convert inputted text to a list of word indices\n",
    "text = text.split()\n",
    "X_user = [word_indices[word] for word in text]\n",
    "\n",
    "# Pad X_user to have the same length as our training examples\n",
    "X_user = pad_sequences([X_user], maxlen=X.shape[1], padding='pre')\n",
    "\n",
    "# Repeatedly generate a new word and update X_user to include it\n",
    "for i in range(800):\n",
    "    # Prediction on X_user\n",
    "    y_pred = model.predict(X_user)\n",
    "\n",
    "    # Sample from y_pred and convert to word\n",
    "    sample = np.random.choice(vocab_size, 1, p = y_pred.ravel())\n",
    "    next_word = indices_word[sample[0]]\n",
    "    \n",
    "    # Update X_user to include new word\n",
    "    X_user = (np.append(X_user[0][1:], (sample[0]))).reshape(1,-1)\n",
    "    \n",
    "    # Print sampled word\n",
    "    if next_word == '<eos>':\n",
    "        sys.stdout.write('.')\n",
    "    elif next_word == '<comma>': \n",
    "        sys.stdout.write(',')\n",
    "    elif next_word == '<semicolon>': \n",
    "        sys.stdout.write(';')\n",
    "    elif next_word == '<question>': \n",
    "        sys.stdout.write('?')\n",
    "    elif next_word == '<exclamation>': \n",
    "        sys.stdout.write('!')\n",
    "    else:\n",
    "        sys.stdout.write(' ')\n",
    "        sys.stdout.write(next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and the summary for it\n",
    "model.save('mobydick_text_generator.h5')\n",
    "\n",
    "with open('mobydick_text_generator.txt', 'w+') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things that can be done to improve model:\n",
    "Use pre-trained word embeddings.\n",
    "Tune the hyperparameters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
